{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c49ff3",
   "metadata": {},
   "source": [
    "## 문제 7: 버스 승하차 인원 월별 정류장별 인원\n",
    "\n",
    "서울버스 노선별, 정류장별 승하차인원을 수집할 수 있는 ```CardSubwayTime API```가 있다.\n",
    "서울버스 대상 (서울시내, 서울광역, 서울마을)으로, 교통카드(선후불교통카드)를 이용하여\n",
    "매일 버스노선별로 각 정류장에 승/하차한 인원을 월별로 집계하고 있다.\n",
    "\n",
    "```CardSubwayTime API``` 호출에 필요한 인자를 정리하면 다음과 같다.\n",
    " \n",
    "변수명 | 타입 | 설명 \n",
    "-----|-----|-----\n",
    "KEY | String | 발급받은 인증키\n",
    "TYPE | String | xml, xmlf, xls, json 데이터 형식\n",
    "SERVICE | String | 서비스명\n",
    "START_INDEX | Integer | 페이징 시작번호\n",
    "END_INDEX | Integer | 페이징 끝번호\n",
    "USE_MON | String | 사용월\tYYYYMM형식의 문자열\n",
    "BUS_ROUTE_NO | String(선택) |\t버스노선번호"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f819778",
   "metadata": {},
   "source": [
    "\t4479426153736b6539356d70667943"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90663802",
   "metadata": {},
   "source": [
    "### 정규식으로 XML 파싱\n",
    "\n",
    "XML 파싱을 통해, 원하는 결과를 추출할 수 있다.\n",
    "여기서는 정규식으로 해보자.\n",
    "<BUS_STA_NM>(.+?)</BUS_STA_NM>는 태그 안의 괄호안 패턴을 추출하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4ef5641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src import mylib\n",
    "\n",
    "keyPath=os.path.join(os.getcwd(), 'src', 'key.properties')\n",
    "key=mylib.getKey(keyPath)\n",
    "_key=str('4479426153736b6539356d70667943')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "912e9828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://openAPI.seoul.go.kr:8088/{'dataseoul': '46416e4957736b653132314d445a6347'}/xml/CardBusTimeNew/1/5/202107/7016\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "busstopurl='http://openAPI.seoul.go.kr:8088/'+_key+'/xml/CardBusTimeNew/1/5/202107/7016'\n",
    "data=requests.get(busstopurl).text\n",
    "print(busstopurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9d475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bfd3a7",
   "metadata": {},
   "source": [
    "### 정규식으로 XML 파싱\n",
    "\n",
    "XML 파싱을 통해, 원하는 결과를 추출할 수 있다.\n",
    "여기서는 정규식으로 해보자.\n",
    "<BUS_STA_NM>(.+?)</BUS_STA_NM>는 태그 안의 괄호안 패턴을 추출하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "251435bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "은평공영차고지\n",
      "월드컵경기장북측\n",
      "상암초등학교\n",
      "월드컵파크2단지.에스플렉스센터\n",
      "월드컵파크3단지.난지천공원\n"
     ]
    }
   ],
   "source": [
    "p=re.compile('<BUS_STA_NM>(.+?)</BUS_STA_NM>')\n",
    "res=p.findall(data)\n",
    "for item in res:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd01d4",
   "metadata": {},
   "source": [
    "```<.*_NUM>(\\d+)</.*_NUM>```는 _NUM 태그 안의 숫자 데이터를 추출하는 정규식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "590bf59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0, 0, 0, 0, 0, 0, 0, 23, 1, 11, 0, 28, 0, 119, 1, 85, 0, 53, 1, 23, 0, 29, 0, 28, 1, 18, 0, 26, 0, 26, 2, 31, 1, 76, 1, 43, 3, 27, 2, 12, 2, 10, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 29, 0, 3, 1, 9, 6, 23, 75, 47, 125, 27, 35, 33, 33, 19, 38, 25, 29, 24, 48, 15, 61, 39, 44, 56, 56, 49, 68, 69, 101, 58, 25, 18, 33, 27, 25, 21, 21, 0, 0, 3, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 13, 192, 39, 763, 27, 435, 12, 282, 26, 238, 25, 212, 35, 182, 27, 227, 19, 246, 31, 214, 18, 254, 30, 319, 23, 265, 31, 189, 24, 127, 21, 158, 6, 70, 1, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 6, 576, 9, 2095, 5, 1215, 14, 522, 13, 295, 9, 275, 17, 297, 8, 261, 9, 257, 18, 220, 22, 212, 85, 232, 30, 244, 26, 150, 10, 139, 26, 168, 2, 70, 0, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 2, 32, 9, 52, 7, 46, 5, 41, 4, 56, 8, 47, 10, 70, 10, 71, 11, 79, 8, 47, 7, 64, 7, 79, 3, 82, 3, 58, 2, 52, 2, 66, 0, 27, "
     ]
    }
   ],
   "source": [
    "p=re.compile('<.*_NUM>(\\d+)</.*_NUM>')\n",
    "res=p.findall(data)\n",
    "#print(\" \".join(res))\n",
    "for item in res:\n",
    "    print (item, end=\", \") # connecting numbers with a comma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f3bec0",
   "metadata": {},
   "source": [
    "## 문제 8: 서울시 골목상권 분석\n",
    "\n",
    "어떤 곳에 입점을 할 경우, 배후 상권에 대해 알 필요가 있다.\n",
    "주변에 얼마나 많은 사람들이 다니는지,\n",
    "어떤 직장이 있는지,\n",
    "관련한 점포는 있는지가 궁금할 수 있다.\n",
    "이러한 골목상권에 대한 정보를 수집할 수 있다.\n",
    "제공하는 골목상권분석정보는:\n",
    "- 서울시 우리마을가게 상권분석서비스(상권-생활인구)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권배후지-집객시설)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권배후지-생활인구)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권-집객시설)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권배후지-상주인구)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권_상주인구)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권배후지-아파트)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권배후지-점포)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권-상권변화지표)\n",
    "- 서울시 우리마을가게 상권분석서비스(행정동별 상권변화지표)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권-추정매출)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권-직장인구)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권배후지-소득소비)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권배후지-직장인구)\n",
    "- 서울시 우리마을가게 상권분석서비스(자치구별 상권변화지표)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권-아파트)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권영역)\n",
    "- 서울시 우리마을가게 상권분석서비스(상권-점포)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469f149",
   "metadata": {},
   "source": [
    "# 아래부터는 공공 데이터 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b839d",
   "metadata": {},
   "source": [
    "## 문제 9: 공공데이터포털 서울시 구별 교통사고 사망자 수\n",
    "\n",
    "공공데이터포털에서 사망교통사고 ```AccidentDeath API```를 사용해서 교통사고 사망자 수를 구해보자.\n",
    "이 데이터는 1년에 1회 갱신되는, 년별 집계를 한다.\n",
    "사망교통사고 API는 다음 2가지가 있다.\n",
    "- ```getRestTrafficAccidentDeath``` 사망교통사고정보 Rest 조회\n",
    "- ```getWMSTrafficAccidentDeath``` 사망교통사고정보 WMS 조회 (PNG 이미지를 조회)\n",
    "\n",
    "```AccidentDeath API``` 호출에 필요한 인자는 다음과 같다.\n",
    "\n",
    "변수명 | 설명 \n",
    "-----|-----\n",
    "serviceKey | 공공데이터포털에서 발급받은 인증키\n",
    "searchYear | 조회하려는 연도 (예: 2019)\n",
    "siDo | 법정동 시도코드\n",
    "guGun | 법정동 시군구코드\n",
    "type | 결과형식 (xml, json)\n",
    "numOfRows | 한 페이지 결과 수 (Default: 10)\n",
    "pageNo | 페이지 번호 (Default: 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2d760",
   "metadata": {},
   "source": [
    "## 문제 10: 공공데이터포털 대기 오염 정보\n",
    "\n",
    "공공데이터포털에서 대기오염을 검색하면 ```한국환경공단_에어코리아_대기오염정보```를 찾을 수 있다.\n",
    "이 API를 사용하려면, ```한국환경공단_에어코리아_대기오염정보``` 항목을 선택하여 활용신청'한다.\n",
    "\n",
    "변수명 | 설명 \n",
    "-----|-----\n",
    "serviceKey | 공공데이터포털에서 발급받은 인증키\n",
    "returnType (선택) | 데이터표출방식, xml 또는 json\n",
    "numOfRows (선택) | 한 페이지 결과 수 (Default: 10)\n",
    "pageNo (선택) | 페이지 번호 (Default: 1)\n",
    "searchDate (선택) | 조회날짜 (예: 2020-11-14)\n",
    "InformCode (선택) | 통보코드검색 (PM10, PM25, O3) (예: PM10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c03c92",
   "metadata": {},
   "source": [
    "# 여기부터 spark RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79459a4f",
   "metadata": {},
   "source": [
    "#### SparkSession 생성\n",
    "\n",
    "\n",
    "Spark를 사용하려면 ```SparkSession``` 객체를 생성해야 한다.\n",
    "\n",
    "SparkSession을 생성해 보자. SparkSesion은 sql 모듈로 **'pyspark.sql.SparkSession'**을 클라이언트로 사용한다.\n",
    "필요한 설정은 SparkSession이 만들지기 전에 해 두어야 한다. 여기서는 설정을 별도로 하지 않고 비워 놓았다.\n",
    "SparkSession은 **builder.getOrCreate()** 함수를 호출하여, 기존의 session 또는 새로 생성하여 사용한다. 함수 **getOrCreate()** 함수는 **singleton 패턴**으로 한 번에 하나의 세션만이 존재하도록 한다.\n",
    "SparkSession을 종료하려면 stop() 함수를 호출한다.\n",
    "\n",
    "```python\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010b1f1",
   "metadata": {},
   "source": [
    "Spark를 실행하기 전 **필수적**으로 설정해야 하는 항목은\n",
    "* **master**: (1) 분산의 경우 master URL 또는 (2) 로컬인 경우 ```local[]```라고 적어준다.\n",
    "즉 local의 수는 CPU core의 수를 의미한다. 예를 들어 ```local[*]```는 가능한 최대한의  core를 사용한다는 의미이다.\n",
    "예를 들어, local[5]라고 하면, core의 수가 2개라고 하더라도 데이터는 5개의 partitions로 나누어져 주어진다.\n",
    "    * ```local```은 Spark를 로컬에서 실행한다는 의미이다.\n",
    "    * ```local[n]```는 worker의 쓰레드를 n개로 한다는 의미. CPU core의 개수에 맞추어 설정하자.\n",
    "    * ```local[*]``` 는 가능하면 가용한 모든 쓰레드를 사용한다는 의미 (Runtime.getRuntime.availableProcessors()로 그 수를 알 수 있다)\n",
    "* **appName**: 앱의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f3e385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767201df",
   "metadata": {},
   "source": [
    "spark 변수를 만들어야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d6bff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1f29e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version \t: 3.1.2\n",
      "Spark App \t: myApp\n",
      "Spark Master \t: local\n",
      "Spark Host \t: 172.30.1.26\n"
     ]
    }
   ],
   "source": [
    "print (\"Spark version \\t: {}\".format(spark.version))\n",
    "print (\"Spark App \\t: {}\".format(spark.conf.get('spark.app.name')))\n",
    "print (\"Spark Master \\t: {}\".format(spark.conf.get('spark.master')))\n",
    "print (\"Spark Host \\t: {}\".format(spark.conf.get('spark.driver.host')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c6154",
   "metadata": {},
   "source": [
    "# spark가 생성되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccdaa4",
   "metadata": {},
   "source": [
    "## S.3 데이터 구조\n",
    "\n",
    "Spark에서는 RDD, Dataframe, DataSet 세 가지 데이터구조를 제공하고 있다. 이 가운데 **RDD API에 대한 지원은 축소**되고 있다.\n",
    "* **RDD**는 Spark 1.0부터 사용되었고, 이를 기반으로 다른 데이터구조가 만들어졌다. RDD는 데이터가 **비구조적**인 경우 사용하기 적합하다. 모델schema를 정하지 않고 사용할 수 있다.\n",
    "* **Dataframe**은 버전 1.3에서 제공되어 많이 쓰이고 있다. DataFrame과 DataSet은 데이터가 schema와 데이터타잎을 가진 **구조적**인 경우 사용한다.\n",
    "\n",
    "\n",
    "데이터구조 | 언제 Spark에 도입 | 설명\n",
    "---------|---------|---------\n",
    "**RDD** | 1.0 | **비구조적**, schema free, low-level\n",
    "**Dataframe** | 1.3 | **구조적**, schema를 가진다. Dataset[Row]와 같은 의미로, 타잎을 강제하지 않는다.\n",
    "**Dataset** | 1.6 | 자바의 Generic과 같이 Dataset[T]으로 '타잎'을 강제하는 형식이다. Scala and Java에서 사용한다. Python loosely-typed이므로 사용하지 않는다.\n",
    "\n",
    "* Spark의 RDD, DataFrame 모두 immutable이라 일단 생성되고 나면 원본을 수정할 수 없다.\n",
    "* Spark의 데이터는 모두 **lazy**, 실제 transformation을 action까지 연기한다. **변환할 때마다 실제 변환이 일어나면 그 결과가 메모리에 저장되는 비효율성**을 막기 위해, **action이 실행되는 경우, 계산이 이루어지고, 실제 메모리를 사용**한다. RDD의 경우, action이 실행될 때마다 재계산이 이루어지는 것을 막기 위해 persist (or cache)함수를 사용할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e15a5",
   "metadata": {},
   "source": [
    "## S.4 RDD 소개\n",
    "\n",
    "RDD (Resilient Distributed Dataset)는 그 줄임말에서 알 수 있듯이 **데이터를 저장하고 있는 DataSet**이며, 여러 컴퓨터에 **분산**해서 사용해서 사용할 수 있다는 점이 특징이다.\n",
    "* Resilient - 작업이 실패하지 않도록 fault tolerent, 즉 어느 한 노드에서 작업이 실패하면 다른 노드에서 실행\n",
    "* Distributed - 클라스터로 구성된 여러 노드에 분산해서 처리\n",
    "* Dataset - 데이터 구조\n",
    "\n",
    "RDD는 Python List, 파일, hdfs 등 다양한 자료에서 생성할 수 있고, 생성된 자료는 수정할 수 없는 read-only이다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c1a64",
   "metadata": {},
   "source": [
    "## S.5 RDD 생성\n",
    "\n",
    "RDD는 **sparkContext**로부터 만들어 진다.\n",
    "* 1) 이미 만들어진 배열과 같에서 읽어서 생성한다. 이 경우 parallelize() 함수를 사용하게 된다.\n",
    "* 2) 또는 파일, 데이터베이스 등 외부에서 읽어서 생성할 수도 있다. textFile() 함수를 사용한다.\n",
    "\n",
    "생성 방법 | 설명 | 함수\n",
    "----------|----------|----------\n",
    "내부에서 읽기 | Pytho list에서 생성 | parallelize()\n",
    "외부에서 읽기 | 파일, HDFS, HBase 등 | ```textFile(\"mydir/\")```<br>```textFile(\"mydir/*.txt\")```<br>```textFile(\"mydir/*.gz\")```<br>```Hadoop InputFormat```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0054b3",
   "metadata": {},
   "source": [
    "### List에서 RDD 생성하기\n",
    "\n",
    "```sparkContext.parallelize()``` 함수를 사용하여 Python list에서 RDD를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aac070c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9feb8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd1 = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56e5fe",
   "metadata": {},
   "source": [
    "take(number)는 num만큼의 줄을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eee06db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517e2b9",
   "metadata": {},
   "source": [
    "### 파일에서 RDD 생성하기\n",
    "\n",
    "파일에서 직접 RDD를 생성해 본다.\n",
    "현재 작업 디렉토리 아래에 **'data/' 디렉토리**를 만들고 아래 파일을 생성한다.\n",
    "파일 내용은 wikipedia에서 Apache spark를 검색한 후 첫 문단을 복사해서 가져 왔다.\n",
    "일부러 3째줄은 한글, 4째 줄은 같은 단어를 반복해서 추가했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "720b19f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365baf79",
   "metadata": {},
   "source": [
    "파일에서 RDD를 생성하기 위해서는 앞서와 같이 SparkContext를 사용한다.\n",
    "파일명을 textFile() 함수 인자로 넣어서 만들어 주면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a4575",
   "metadata": {},
   "source": [
    "외부에서 생성할때는 textFile이다. \n",
    "\\는 명령어가 두번째 줄로 만들어진다는 뜻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "febc02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769afac5",
   "metadata": {},
   "source": [
    "```first()```는 첫 데이터만 조회하는 action함수이다.\n",
    "first()은 take(1)과 동일한 결과를 출력하는데, 그 이유는 first()는 내부적으로 take(1) 함수를 사용하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7ab7bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wikipedia'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280cd87",
   "metadata": {},
   "source": [
    "#### RDD와 Spark Dataframe를 만드는 함수는 서로 다르다\n",
    "\n",
    "DataFrame은 다음 장에서 배우게 되겠지만, file에서 읽는 방식이 RDD와 Dataframe이 서로 다르다.\n",
    "RDD는 sparkContext.textFile(), Dataframe은 read.text()을 사용한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "SparkSession.sparkContext.textFile() | **'SparkContext'를 사용하므로 RDD를 생성**한다.\n",
    "SparkSession.read.text() | **DataFrame을 생성**한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79247d",
   "metadata": {},
   "source": [
    "os.path.join ->운영체제 독립적으로 경로명을 합성해주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7e2616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value='Wikipedia')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "print (myDf.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8aae5",
   "metadata": {},
   "source": [
    "읽고 생성된 변수 myDf의 데이터타입을 type()으로 확인하면 DataFrame이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e060b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print (type(myDf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89855c8",
   "metadata": {},
   "source": [
    "#### csv에서 RDD 생성하기\n",
    "\n",
    "csv 파일은 컴마로 구분된 데이터를 저장하고 있다. 이 파일을 읽어서 RDD를 생성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea71e329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434e537",
   "metadata": {},
   "source": [
    "파일에서 읽어 RDD를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce39a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c1b3c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "myList=myRdd4.take(5)\n",
    "print (type(myList))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d23843",
   "metadata": {},
   "source": [
    "## S.6 RDD API\n",
    "\n",
    "앞서 RDD를 생성하여 보았다.\n",
    "이제는 RDD라는 데이터구조에서 데이터를 읽고 변환하고 분석하여 보자.\n",
    "RDD는 **데이터 변환Transformations**, **연산Actions**으로 구분할 수 있다.\n",
    "다음에 배우게 될 **Dataframe**의 **Transformer**, **Estimator**와 비교될 수 있다.\n",
    "\n",
    "### 변환 **transformations**\n",
    "\n",
    "변환함수는 **lazy연산**을 한다. 실제 변환은 action이 수행되는 시점까지 늦추어져서 이루어진다.\n",
    "변환 결과는 RDD 또는 seq(RDD)로 만들어진다.\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "```map(fn)``` | 요소별로 fn을 적용해서 결과 RDD 돌려줌 | ```.map(lambda x: x.split(' ')```\n",
    "```filter(fn)``` | 요소별로 선별하여 fn을 적용해서 결과 RDD 돌려줌 | ```.filter(lambda x: \"Spark\" in x)```\n",
    "```flatMap(fn)``` | 요소별로 fn을 적용하고, flat해서 결과 RDD 돌려줌 | ```.flatMap(lambda x: x.split(' '))```\n",
    "```groupByKey()``` | key를 그룹해서 iterator를 돌려줌. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f08851",
   "metadata": {},
   "source": [
    "flatmap -> map 함수와 다르다 \n",
    "2중구조 가지고 있을때 그 구조 없애서 처리한다. 납작하게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9051414b",
   "metadata": {},
   "source": [
    "### **actions**\n",
    "\n",
    "RDD를 값으로 변환하는데, 보통 Python 리스트가 생성된다.\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "```reduce(fn)``` | 요소별로 fn을 사용해서 줄여서 결과 list를 돌려줌 | ```reduce(lambda x,y:x+y)```\n",
    "```collect()``` | 모든 요소를 결과 list로 돌려줌 |\n",
    "```count()``` | 요소의 갯수를 결과 list로 돌려줌 |\n",
    "```take(n)``` | ```collect()```는 전체이지만, n개만 돌려줌 | ```take(1)```\n",
    "```countByKey()``` | key별 갯수를 세는 함수 | ```countByKey().items()```\n",
    "```foreach(fn)``` | 각 데이터 항목에 함수fn을 적용 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e307ac5",
   "metadata": {},
   "source": [
    "###  S.6.1 우선 Python 함수로 해보기\n",
    "\n",
    "#### map()함수를 사용하지 않고 해보기\n",
    "\n",
    "map()을 사용하지 않고 섭씨를 화씨로 변환하는 **c2f()**함수를 만들어 보자.\n",
    "* 데이터를 하나씩 읽어 **for문으로 처리하고, 리스트로 만들어 반환**한다.\n",
    "* 나중에 **map()함수를 사용하면 for문이 없어도** 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48501352",
   "metadata": {},
   "source": [
    "map을 쓰면 for문 없어진다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2dd53510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print (c2f(celsius))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf90bd",
   "metadata": {},
   "source": [
    " #### map() 함수를 사용해 보기\n",
    "\n",
    "Python은 map(), reduce(), filter() 함수를 이미 가지고 있다.\n",
    "Python map()을 사용해 보자. **for문이 사라진다**는 점에 유의한다.\n",
    "반복문이 외관상으로는 보이지 않지만, map() 함수 내부적으로 실행된다는 점에 유의하자.\n",
    "**map() 함수의 인자는 2개** 이다.\n",
    "* (1) 첫째 인자는 **처리 함수**이고, **함수의 return은 반드시 있어야** 한다,\n",
    "* (2) 두 번째 인자는 **처리하려는 데이터**이다.\n",
    "\n",
    "함수 | 설명 | 예\n",
    "-------|-------|-------\n",
    "map() | 각 데이터 요소에 함수를 적용해서 list를 반환 | **map(fn,data)**\n",
    "filter() | 각 데이터 요소에 함수의 결과 True를 선택해서 반환 | **filter(fn, data)**\n",
    "reduce() | 각 데이터 요소에 함수를 적용해서 list를 반환 | **reduce(fn, data)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79dd4263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "f=map(c2f, celsius)\n",
    "print (list(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6795810",
   "metadata": {},
   "source": [
    "* lambda 함수의 설명\n",
    "lambda는 익명함수이다. 함수의 인자를 받아서, 한 줄의 표현식으로 처리한다.\n",
    "\n",
    "```python\n",
    "lambda 함수인자 : 표현식\n",
    "```\n",
    "\n",
    "**lambda함수**를 사용하면 줄 수가 많이 줄게 된다. lambda는 무명 함수이므로 함수 선언이 별도 필요 없다. 처리 결과는 **'return'을 사용하지 않아도 반환**된다.\n",
    "\n",
    "* lambda는 함수를 정의하는 명령어로서, 이름이 없다는 특징이 있다. 매우 직관적으로 사용할 수 있게 된다.\n",
    "* map-reduce 함수에서 자주 사용되므로 잘 이해해야 한다.\n",
    "* 단순히 인자에 2를 곱하는 lamdba함수 'y'를 정의하면 다음과 같다. 같은 기능을 lambda를 사용하지 않고, 기존 함수 방식으로 만들면 프로그램이 조금 더 늘어나게 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b30779ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y = lambda x : x * 2\n",
    "print (y(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c9f650a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x * 2\n",
    "y=f(1)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9aa907",
   "metadata": {},
   "source": [
    "또는 인자를 2개 가질 수도 있다.\n",
    "x, y인자를 받아 그 합산을 반환하는 lambda 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57b2fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x = lambda x, y : x + y\n",
    "print(x(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55a5b2",
   "metadata": {},
   "source": [
    "처음 map함수 결과가 object로 들어가서 list로 바꿔줘야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01aff01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "f=map(lambda c:(float(9)/5)*c + 32, celsius)\n",
    "print(list(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b302f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello World\"\n",
    "list(map(lambda x:x.split(), sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d15d1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello', 'World'], ['Good', 'Morning']]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"Hello World\", \"Good Morning\"]\n",
    "a = list(map(lambda x:x.split(), sentence))\n",
    "\n",
    "print(list(map(lambda x:x.split(), sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c860f32",
   "metadata": {},
   "source": [
    "#### filter()\n",
    "\n",
    "filter()는 데이터를 선별한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979bc5af",
   "metadata": {},
   "source": [
    "x를 2로 나눈것의 나머지가 0 혹은 1 이된다. \n",
    "따라서 filter함수를 거쳐서 나머지가 1인 홀수만 적어준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87ea7164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x % 2, fib)\n",
    "print (list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c732498",
   "metadata": {},
   "source": [
    "#### reduce()\n",
    "\n",
    "reduce() 역시 **함수와 데이터 2개의 인자**를 받는다.\n",
    "데이터에 대해 함수를 반복적으로 적용하여 결과 값을 만들게 된다, 즉 [ func(func(s1, s2),s3), ... , sn ]와 같이 수행한다. 아래 예는 1부터 101까지 **두 수 x,y 인자를 반복해서 더한다**는 것이다.\n",
    "x는 부분합계로 y를 계속 저장해 나가는 역할을 하며, 최종 합계에 이르게 된다.\n",
    "\n",
    "단계 | x<p>부분 합계 | y | 함수적용\n",
    "-----|-----|-----|-----\n",
    "1 | 0 | 1 | func(0,1)\n",
    "2 | 1 | 2 | func(1,2) <-- func(0 + 1, 2)\n",
    "3 | 3 | 3 | func(3,3) <-- func(0 + 1 + 2, 2)\n",
    "4 | 6 | 4 | func(6,4) <-- func(0 + 1 + 2 + 3, 2)\n",
    "...|...|...|...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2d786",
   "metadata": {},
   "source": [
    "range(1,101) 뒤에 101은 포함 안된다.\n",
    "x는 부분합계이고 y는 1부터 100까지 정수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cab91d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "reduce(lambda x, y: x + y, range(1,101))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630a627",
   "metadata": {},
   "source": [
    "## S.6.2 RDD 사용하기\n",
    "\n",
    "transformation, action 함수를 사용해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f83a5",
   "metadata": {},
   "source": [
    "### map\n",
    "\n",
    "* 요소별 제곱\n",
    "\n",
    "map()을 사용해서 각 요소를 **제곱**해 보자.\n",
    "리스트의 요소를 하나씩 가져와서 제곱을 하게 된다.\n",
    "map()은 transformation 함수라서, 실제 값은 action 함수가 적용될 때까지 연기되어 계산된다.\n",
    "map() 함수의 반환 값은 RDD이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f9bdab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[16] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "print (squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbac944",
   "metadata": {},
   "source": [
    "변환의 실제 결과를 보려면 collect()를 사용해서 출력해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18bfd71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "print (squared.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6f81c",
   "metadata": {},
   "source": [
    "* csv를 배열로\n",
    "\n",
    "위에서 csv 파일을 읽어서 아래와 같이 구성된 요소를 정수 리스트로 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5dc9a",
   "metadata": {},
   "source": [
    "myRdd4는 위에 생성한 csv파일이다\n",
    "리스트이다. 각각의 요소가 ''로 묶여졌다. (5개로 묶여져있다.)\n",
    "형변환을 나중에 시켜줘야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7e62586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['35, 2', '40, 27', '12, 38', '15, 31', '21, 1']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b32002",
   "metadata": {},
   "source": [
    "2차원 리스트로 만드려면, map()함수를 사용한다.\n",
    "map() 함수에는 아래와 같은 로직이 숨어있다.\n",
    "\n",
    "```python\n",
    "모든 줄을 반복:\n",
    "    한 줄line을 읽는다.\n",
    "    줄line을 컴마(,)로 분리한다.\n",
    "    줄line을 리스트로 만든다.\n",
    "```\n",
    "\n",
    "map() 함수에 lambda를 넣어서 해보자.\n",
    "line을 받아서 split(,) 즉 컴마로 분리하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09276d44",
   "metadata": {},
   "source": [
    "2차원 리스트로 다시 만들어졌다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "10f88dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['35', ' 2'], ['40', ' 27'], ['12', ' 38'], ['15', ' 31'], ['21', ' 1']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd5 = myRdd4.map(lambda line: line.split(','))\n",
    "myRdd5.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c2362",
   "metadata": {},
   "source": [
    "* 문자열을 정수로 변환\n",
    "\n",
    "결과에서 보듯이, 리스트 안의 숫자들이 따옴표로 되어 있다.\n",
    "이는 숫자가 아니라 문자라는 의미이다. 문자는 + 연산을 하면 아래와 같이 합성이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a424946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'352'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'35'+'2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452053a",
   "metadata": {},
   "source": [
    "뭔가 연산을 하려면, 숫자를 정수로 형변환을 해야 한다.\n",
    "우선 리스트의 첫 째 요소 ['35', '2']를 Python으로 형변환 해보자.\n",
    "이 경우 반복문으로 하나씩 읽어서 문자를 int()로 형변환을 해야 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "687a57dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 2]\n"
     ]
    }
   ],
   "source": [
    "x=['35', '2']\n",
    "y=list()\n",
    "for i in x:\n",
    "    y.append(int(i))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa762a",
   "metadata": {},
   "source": [
    "이를 줄여서 아래와 같이 처리하는데, 프로그램이 줄 수가 줄어들어 간략해 지면서 Python의 특징이 잘 드러나고 있다.\n",
    "이를 \n",
    "### Pythonoic하다라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a4aaf419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 2]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=['35', '2']\n",
    "[int(i) for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be669db",
   "metadata": {},
   "source": [
    "잘 되는 것을 확인했으니, map() 함수에 넣어서 정수로 만들자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53777453",
   "metadata": {},
   "source": [
    "* 단어 분리\n",
    "\n",
    "map 함수를 사용해서 문ㅁ서를 문장으로 분리해 보자.\n",
    "문서파일이 10개 문장을 포함하고 있으므로, count()는 10개를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d81eaf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089cd55c",
   "metadata": {},
   "source": [
    "```str.split()``` 함수에 인자가 없으면 whitespace로 분리한다. whitespace는 공백이나 탭 등의 기호를 말하는 것으로, 문장을 분리해서 단어로 분리할 경우에 사용한다.\n",
    "\n",
    "> whitespace\n",
    "\n",
    "> 스페이스바, 탭, RETURN, ENTER 키를 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e0dc815",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=myRdd2.map(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f48e3",
   "metadata": {},
   "source": [
    "문서는 문장으로 문장은 단어로 구성되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d22f9d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457b886",
   "metadata": {},
   "source": [
    "* 사용자 함수를 이용해서 단어 분리\n",
    "\n",
    "사용자 함수, 즉 사용자가 만든 mySplit() 함수를 사용해 map()을 수행하고 있다.\n",
    "이와 같이 lambda를 사용하지 않고, 사용자함수로 map()을 사용할 수 있다.\n",
    "lambda는 한 줄의 명령문만 가지게 되므로, 여러 명령문으로 함수를 만드는 경우 사용자함수를 만들어 유용하게 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ac2860a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mySplit(x):\n",
    "    return x.split()\n",
    "sentences2=myRdd2.map(mySplit)\n",
    "sentences2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ec7f810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Wikipedia'],\n",
       " ['Apache',\n",
       "  'Spark',\n",
       "  'is',\n",
       "  'an',\n",
       "  'open',\n",
       "  'source',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'framework.'],\n",
       " ['아파치', '스파크는', '오픈', '소스', '클러스터', '컴퓨팅', '프레임워크이다.']]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da02a6",
   "metadata": {},
   "source": [
    "* 중첩 for 문으로 2차원 배열의 출력\n",
    "\n",
    "앞서 본 경우와 같이 리스트에는 **문장**이 분리된 **단어**가 요소로 담겨져 있으므로, **for문을 중첩**해서 사용해야 단어를 출력할 수 있다.\n",
    "Python3에서는 print()가 함수로 취급된다. 단어를 출력할 경우, 그 사이에 공백을 넣고 싶으면 end= ' ' 라고 적어주어야 한다.\n",
    "따옴표는 한 개 '' 또는 \"\" 어느 것으로 해도 기능의 차이는 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97c5ff",
   "metadata": {},
   "source": [
    "첫번째 for문 문서가 문장으로 쪼개짐\n",
    "\n",
    "두번째 for문은 문장이 단어로 쪼개짐 end때문에 공백으로 연결댐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27e91cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-----\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-----\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-----\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark \n",
      "-----\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "-----\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "-----\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "-----\n",
      "which has maintained it since. \n",
      "-----\n",
      "Spark provides an interface for programming entire clusters with \n",
      "-----\n",
      "implicit data parallelism and fault-tolerance. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for line in sentences.collect():\n",
    "    for word in line:\n",
    "        print (word, end=\" \")\n",
    "    print (\"\\n-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b91fd",
   "metadata": {},
   "source": [
    "* 문자 개수\n",
    "\n",
    "각 문장의 철자 갯수를 세어 보자.\n",
    "철자는 len()함수를 사용하면 된다.\n",
    "첫 문장 'Wikipedia'는 **9**, 다음 문장 'Apache Spark is an open source cluster computing framework.'는 마침표를 포함하여 **59**자를 출력하고 있다.\n",
    "아래는 58을 출력하는데, 자세히 살펴보면 마침표가 빠져있어서 그렇다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb7f4380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Apache Spark is an open source cluster computing framework.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d1ed0",
   "metadata": {},
   "source": [
    "문자 개수를 센 후, 배열로 만들어보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "26a9e705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 31, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305a258",
   "metadata": {},
   "source": [
    "* 교체\n",
    "\n",
    "이번에는 리스트에서 RDD를 만들어 간단한 문자처리 기능으로 **대소문자 변환**이나 **교체**를 해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5139cc",
   "metadata": {},
   "source": [
    "리스트에서 rdd만들기 sparkcontext.parralelize\n",
    "\n",
    "파일에서 rdd 만들기 sparkcontext.textfile(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e8bb0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[\"this is\",\"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c6b44e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "wordsRdd=_rdd.map(lambda x:x.split())\n",
    "print (wordsRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "10094f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repRdd=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "repRdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85166415",
   "metadata": {},
   "source": [
    "* 대소문자 변환\n",
    "\n",
    "첫 글자를 대문자로 만들어서 출력해 보자.\n",
    "다음 's'.upper()는 철자 's'를 대문자로 출력하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2989795a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s'.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1c0b6",
   "metadata": {},
   "source": [
    "첫째 요소 **x[0]**는 리스트 각 요소의 첫째 단어 'this', 'a'를 대문자로 변환하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "757770a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "upperRDD =wordsRdd.map(lambda x: x[0].upper())\n",
    "print (upperRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38172783",
   "metadata": {},
   "source": [
    "리스트의 모든 단어를 대문자로 바꾸려면 for문을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3a2bc050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "upper2RDD =wordsRdd.map(lambda x: [i.upper() for i in x])\n",
    "print (upper2RDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594af1ae",
   "metadata": {},
   "source": [
    "### reduce\n",
    "\n",
    "앞서 RDD를 사용하지 않고, Python reduce()한 예와 비교해 보자.\n",
    "reduce()는 lamdba함수를 사용해서 **입력 데이터를 하나씩 서로 더해서 x+y** 결과 값을 만들어 낸다.\n",
    "\n",
    "```python\n",
    "subtotal = 0으로 초기화,\n",
    "subtotal = subtotal + 1 (y는 1이므로)\n",
    "subtotal = subtotal + 2 (y는 하나 증가해서 2이므로)\n",
    "...\n",
    "subtotal = subtotal + 100 (y는 하나씩 증가해서 최종 값 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077362a0",
   "metadata": {},
   "source": [
    "1부터 100까지 합 구하기\n",
    "\n",
    "1. 반복분\n",
    "\n",
    "2. 재귀함수 \n",
    "\n",
    "3. reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7d7e5526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))\n",
    "myRdd100.reduce(lambda subtotal, x: subtotal + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704b99d",
   "metadata": {},
   "source": [
    "### 단순 통계 기능\n",
    "\n",
    "텍스트데이터와 달리 정량데이터로부터 sum, min, max, 표준편차 등 서술통계를 계산해 낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "010d684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum:  5050\n",
      "min:  1\n",
      "max:  100\n",
      "standard deviation: 28.86607004772212\n",
      "variance:  833.25\n"
     ]
    }
   ],
   "source": [
    "print (\"sum: \", myRdd100.sum())\n",
    "print (\"min: \", myRdd100.min())\n",
    "print (\"max: \", myRdd100.max())\n",
    "print (\"standard deviation:\", myRdd100.stdev())\n",
    "print (\"variance: \", myRdd100.variance())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89efaa4f",
   "metadata": {},
   "source": [
    "### filter()\n",
    "\n",
    "filter() 함수로 조건에 맞는 문장만 분리해 보자.\n",
    "\"Spark\" 단어가 포함된 문장이 조건이된다.\n",
    "count() 함수로 그 갯수를 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d567560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lines having 'Spark':  4\n"
     ]
    }
   ],
   "source": [
    "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line)\n",
    "print (\"How many lines having 'Spark': \",myRdd_spark.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eec9ed",
   "metadata": {},
   "source": [
    "유니코드 일때는 u자를 넣어줘야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ea64ee42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\n",
    "print (myRdd_unicode.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eec49d",
   "metadata": {},
   "source": [
    "* filter()를 사용해서 **stopwords** 제거하기\n",
    "\n",
    "문장 안에 stopwords를 포함한 경우는 제거되지 않는다.\n",
    "따라서 flatMap()을 하고 단어에 대해 불용어를 제거해야 한다.\n",
    "불용어는 단어빈도를 계산하면서 제거하고 싶은 단어를 말한다.\n",
    "불용어는 빈도를 세어도 의미가 없는 대명사 (이, 그, 저...) 또는 한 글자 단어 (등...)이 될 수 있다.\n",
    "한글은 유니코드로 처리해야 한다.\n",
    "영어는 대소문자를 모두 처리하기 위해 여기서는 소문자로 만들어 처리한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d2922",
   "metadata": {},
   "source": [
    "flat 2차원 구조를 1차원으로 만들어야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1781a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['is','am','are','the','for','a', 'an', 'at']\n",
    "myRdd_stop = myRdd2.flatMap(lambda x:x.split())\\\n",
    "                    .filter(lambda x: x not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fb785dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Apache Spark open source cluster computing framework. 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. Apache Spark Apache Spark Apache Spark Apache Spark 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 Originally developed University of California, Berkeley's AMPLab, Spark codebase was later donated to Apache Software Foundation, which has maintained it since. Spark provides interface programming entire clusters with implicit data parallelism and fault-tolerance. "
     ]
    }
   ],
   "source": [
    "for words in myRdd_stop.collect():\n",
    "    print (words, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbabc3cf",
   "metadata": {},
   "source": [
    "### foreach()\n",
    "\n",
    "```foreach()```는 action이지만, 다른 action 함수들과 달리 **반환 값이 없다**.\n",
    "각 요소에 대해 적용한다는 역할에 대해 유사한 기능을 하는 ```map()``` 함수가 있다.\n",
    "```map()``` 함수는 각 요소에 대해 계산을 하고, 그 값을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b6dc6334",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cac8959f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0ef46",
   "metadata": {},
   "source": [
    "노트북에서는 안댐 python에서만 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5d37adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1989e60",
   "metadata": {},
   "source": [
    "### pipeline\n",
    "\n",
    "파이프라인은 **transformation**(예: map()), **action**(예: collect()) 함수를 **연이어 적용**하는 방식을 말한다.\n",
    "파이프라인이 아니라면 함수를 하나씩 끝나고, 결과를 받은 후 다음 함수를 단계별로 적용하게 된다.\n",
    "보다 효율적인 처리를 위해 함수들을 파이프라인같이 붙여서 중간결과를 별도로 산출하지 않고 연이어 처리한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2da3ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "upper2list=wordsRdd.map(lambda x: [i.upper() for i in x]).collect()\n",
    "print (type(upper2list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379ca08",
   "metadata": {},
   "source": [
    "### 파일에 쓰기\n",
    "\n",
    "* RDD 저장\n",
    "Spark에서 list는 RDD로 만들어 로컬 파일에 쓰게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a048b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize(upper2list).saveAsTextFile(\"data/ds_spark_wiki_out_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "791dac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ds_spark_wiki_out:\r\n",
      "_SUCCESS   part-00000\r\n",
      "\r\n",
      "data/ds_spark_wiki_out_1:\r\n",
      "_SUCCESS   part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/ds_spark_wiki_out*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0fad011a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['THIS', 'IS']\", \"['A', 'LINE']\"]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd=spark.sparkContext.textFile(\"data/ds_spark_wiki_out\")\n",
    "_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9fe665",
   "metadata": {},
   "source": [
    "방금 작성된 파일을 읽어서 내용을 읽어보는 것도 물론 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0cf105",
   "metadata": {},
   "source": [
    "파일로 쓰려면 ```coalesce()``` 함수를 사용해야 한다. 인자는 partition의 수를 적어 준다.\n",
    "```python\n",
    "_rdd.coalesce(1).saveAsTextFile(\"data/ds_spark_wiki_txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ac69f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "_rdd.map(lambda x: \"\".join(x)).coalesce(1).saveAsTextFile(\"data/ds_spark_wiki_out_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "71323edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'IS']\r\n",
      "['A', 'LINE']\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/ds_spark_wiki_txt/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a52131",
   "metadata": {},
   "source": [
    "### groupBy\n",
    "\n",
    "transformation 함수이다. Paired, unpaired RDD에 모두 사용할 수 있지만, 주로 **unpaired RDD**에 많이 쓰인다.\n",
    "키를 선택하여 사용할 수 있는 점이 장점이다.\n",
    "\n",
    "Paired 데이터는 예를 들면 아래와 같이 key, value 쌍으로 구성된 데이터를 말한다.\n",
    "\n",
    "```python\n",
    "(key=202011111, ('lim', 'computer'))\n",
    "(key=202011112, ('cho', 'computer'))\n",
    "(key=202011113, ('lim', 'business'))\n",
    "```\n",
    "반면에 Unpaired는 key, value 쌍으로 구성이 되지 않은 데이터를 말한다. 아래는 이름, 전공으로 구성된 데이터로 키가 없다.\n",
    "또한 텍스트로 구성된 데이터도 키가 없으므로 unpaired 데이터이다.\n",
    "\n",
    "```python\n",
    "'lim', 'computer'\n",
    "'cho', 'computer'\n",
    "'lim', 'business'\n",
    "```\n",
    "\n",
    "유사한 기능을 수행하는 **groupByKey()와 비교해서 상대적으로 빠르지 않다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2f32bcdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia',\n",
       " 'Apache Spark is an open source cluster computing framework.',\n",
       " '아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " '아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크',\n",
       " \"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'which has maintained it since.',\n",
       " 'Spark provides an interface for programming entire clusters with',\n",
       " 'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd4a5b",
   "metadata": {},
   "source": [
    "* 앞 2글자로 groupBy\n",
    "\n",
    "앞 2글자를 key로 사용해서, groupBy()를 사용할 수 있다.\n",
    "결과는 key-value 쌍으로 만들어졌는데, key는 예상한대로 되었으나 value는 iterator로 생성되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391d200",
   "metadata": {},
   "source": [
    "두글자만 띄어서 그룹바이 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2e561128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wi: <pyspark.resultiterable.ResultIterable object at 0x7f939a06a6a0>\n",
      "Ap: <pyspark.resultiterable.ResultIterable object at 0x7f939a06a2e0>\n",
      "아파: <pyspark.resultiterable.ResultIterable object at 0x7f939a06ab80>\n",
      "Or: <pyspark.resultiterable.ResultIterable object at 0x7f9399e72190>\n",
      "th: <pyspark.resultiterable.ResultIterable object at 0x7f9399e72ca0>\n",
      "wh: <pyspark.resultiterable.ResultIterable object at 0x7f9399e729d0>\n",
      "Sp: <pyspark.resultiterable.ResultIterable object at 0x7f9398e90a60>\n",
      "im: <pyspark.resultiterable.ResultIterable object at 0x7f9398e90460>\n"
     ]
    }
   ],
   "source": [
    "#myRdd_group=myRdd2.flatMap(lambda x:x.split()).groupBy(lambda x:w[0:2])\n",
    "myRdd_group=myRdd2.groupBy(lambda x:x[0:2])\n",
    "\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    print (\"{}: {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4a434",
   "metadata": {},
   "source": [
    "ResultIterable은 반복문으로 해체하여 결과를 출력하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7e35ef73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wi: Wikipedia\n",
      "-----\n",
      "Ap: Apache Spark is an open source cluster computing framework.\n",
      "Ap: Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "-----\n",
      "아파: 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아파: 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "-----\n",
      "Or: Originally developed at the University of California, Berkeley's AMPLab,\n",
      "-----\n",
      "th: the Spark codebase was later donated to the Apache Software Foundation,\n",
      "-----\n",
      "wh: which has maintained it since.\n",
      "-----\n",
      "Sp: Spark provides an interface for programming entire clusters with\n",
      "-----\n",
      "im: implicit data parallelism and fault-tolerance.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#myRdd_group=myRdd2.flatMap(lambda x:x.split()).groupBy(lambda x:w[0:2])\n",
    "myRdd_group=myRdd2.groupBy(lambda x:x[0:2])\n",
    "\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    for eachValue in v:\n",
    "        print (\"{}: {}\".format(k, eachValue))\n",
    "    print (\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d373150",
   "metadata": {},
   "source": [
    "* (key, value)로 구성된 Paired 데이터의 groupBy\n",
    "\n",
    "다음 데이터에 groupBy()를 적용해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b4f0fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"Seoul\",1),(\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043ec1d",
   "metadata": {},
   "source": [
    "우선 SparkContext parallelize() 함수를 사용하여 리스트에서 RDD를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d77b0c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ad8e7",
   "metadata": {},
   "source": [
    "* ResultIterable\n",
    "\n",
    "첫 요소 x[0]는 key이고, 두 개의 key1, key2가 있다. key를 groupBy()로 구분하여 묶어 보자.\n",
    "groupByKey()의 결과는 **```ResultIterable```**이고, 이 객체는 그대로 볼 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "590f395e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul', <pyspark.resultiterable.ResultIterable at 0x7f939a061400>),\n",
       " ('Busan', <pyspark.resultiterable.ResultIterable at 0x7f939a0b9df0>)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e06fe",
   "metadata": {},
   "source": [
    "* mapValues\n",
    "\n",
    "ResultIterable을 리스트로 변환하여 값을 보기로 한다.\n",
    "이 경우 **```mapValues()```** 함수를 사용한다.\n",
    "이 함수를 사용하여, 각 키 (key1, key2)에 대해 **values 값**들의 리스트를 수집하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd8804",
   "metadata": {},
   "source": [
    "mapValues-> key에 대한 value 들어감"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82fb53",
   "metadata": {},
   "source": [
    "또는 보다 간단하게 다음과 같이 ```list```만을 해주어도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e0b75b15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul',\n",
       "  [('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1)]),\n",
       " ('Busan',\n",
       "  [('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1)])]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).mapValues(lambda x: list(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9985d5a",
   "metadata": {},
   "source": [
    "### 5.6.3 Pair RDD\n",
    "\n",
    "Pair RDD는 **key,value 쌍**으로 구성된 RDD를 말한다.\n",
    "이러한 RDD는 키에 대해 연산을 하는 **byKey()** 또는 값에 대해 **byValue()** 함수를 사용할 수 있다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "byKey 연산 | 동일한 키에 대해 연산<br>- 단계 1: key-value를 계산한다. 각 key의 빈도를 계산하여  '(key,1)' 형식으로 만든다.<br>- 단계 2: byKey를 적용한다. 동일한 key의 value를 더해준다.\n",
    "byValue 연산 | 그룹으로 구분하고 나면, 값이 복수 개가 된다. 복수의 값에 대해 개수를 셀지, 합계를 계산할지 mapValues() 함수에서 하게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64e25a",
   "metadata": {},
   "source": [
    "다음과 같이 groupByKey(), reduceBykey(), combineByKey(), aggregateByKey(), mapValues() 함수를 사용할 수 있다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "groupByKey() | 같은 key를 grouping, partition에서 **먼저 reduce하지 않고, 전체로 계산**한다.\n",
    "reduceByKey() | 같은 key의 value를 합계, partition에서 **먼저 reduce하고, 전체로 계산**한다. grouping + aggregation.<p>즉 **reduceByKey = groupByKey().reduce()**\n",
    "combineByKey() | 키별로 합계, 개수 (key, (sum, count))를 계산. createCombiner, mergeValue, mergeCombiners.\n",
    "aggregateByKey() | reduceByKey()와 유사한 기능을 수행한다. 키별로 합계, 개수, 평균을 계산.\n",
    "mapValues() | Paired RDD는 key,value가 있기 마련이고, **value에 적용하는 함수**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baaada8",
   "metadata": {},
   "source": [
    "#### Paired RDD 데이터 예제\n",
    "\n",
    "Partition1, 2, 3으로 데이터가 분할되어 있다고 가정하자.\n",
    "\n",
    "P1 | P2 | P3\n",
    "-----|-----|-----\n",
    "(key1,1)<br> | (key1,1)<br> | (key1,1)<br>\n",
    "(key1,1)<br> | (key2,1)<br> | (key1,1)<br>\n",
    "(key1,1)<br> |              | (key2,1)<br>\n",
    "(key2,1)<br> |              | (key2,1)<br>\n",
    "(key2,1)<br> |              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf347132",
   "metadata": {},
   "source": [
    "#### Paired RDD 생성\n",
    "\n",
    "테스트 데이터를 구성하고, parallelize()함수를 사용하여 RDD를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd4753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
