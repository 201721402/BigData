{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46a04c6",
   "metadata": {},
   "source": [
    "## spark생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f140d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf= pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2da9c2",
   "metadata": {},
   "source": [
    "### List에서 RDD 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f4b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [1,23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d1114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd1= spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a3baeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 23]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1d862",
   "metadata": {},
   "source": [
    "### 파일에서 RDD 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c720a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58467229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "myRdd2 = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81aa4811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia', 'Apache Spark is an open source cluster computing framework.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f24eab",
   "metadata": {},
   "source": [
    "## csv에서 RDD 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa8e04e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e93bcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0ba3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "myList=myRdd4.take(5)\n",
    "print (type(myList))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bed3a2",
   "metadata": {},
   "source": [
    "## S.6 RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14fcf3b",
   "metadata": {},
   "source": [
    "우선 파이썬으로 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6bcaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lambda x : x * 2\n",
    "print (y(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f754e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print (c2f(celsius))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a16f3",
   "metadata": {},
   "source": [
    "# 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4e3d3",
   "metadata": {},
   "source": [
    "# 위에껄 map 함수로 바꿔보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cb85fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    a=(float(9)/5)*c + 32\n",
    "    return a\n",
    "\n",
    "print(list(map(c2f,celsius)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lambda x : x * 2\n",
    "print (y(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41abb30",
   "metadata": {},
   "source": [
    "결과는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lambda x, y : x + y\n",
    "print(x(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2c11e",
   "metadata": {},
   "source": [
    "결과는?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79418903",
   "metadata": {},
   "source": [
    "# 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435cf22",
   "metadata": {},
   "source": [
    "위에 온도계산 문제를 lambda를 이용하여 더짧게 만들어라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87409d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "list(map(lambda x: float(9)/5*x + 32, celsius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello World\"\n",
    "list(map(lambda x:x.split(), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d28f2",
   "metadata": {},
   "source": [
    "결과는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed96b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"Hello World\", \"Good Morning\"]\n",
    "a = list(map(lambda x:x.split(), sentence))\n",
    "\n",
    "print(list(map(lambda x:x.split(), sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074996f",
   "metadata": {},
   "source": [
    "결과는?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fb51de",
   "metadata": {},
   "source": [
    "# filter()\n",
    "\n",
    "filter()는 데이터를 선별한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x % 2, fib)\n",
    "print (list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda0f5e",
   "metadata": {},
   "source": [
    "# reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0416b0",
   "metadata": {},
   "source": [
    "# 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95157f6d",
   "metadata": {},
   "source": [
    "### reduce를 이용해서1부터 100까지의 합을 구하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d578c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "reduce(lambda x, y: x+ y, range(1,101))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba68e1d",
   "metadata": {},
   "source": [
    "# Rdd 사용하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735319e",
   "metadata": {},
   "source": [
    "# map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a0e5a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "print (squared.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac86446",
   "metadata": {},
   "source": [
    "rdd map은  .collect를 해야함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d262a585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['35, 2', '40, 27', '12, 38']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4.take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404fde00",
   "metadata": {},
   "source": [
    "# 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec06986",
   "metadata": {},
   "source": [
    "myRdd4를 컴마로분리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8417940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['35', ' 2'], ['40', ' 27'], ['12', ' 38'], ['15', ' 31']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4.map(lambda x: x.split(',')).take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b06eeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6180522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia',\n",
       " 'Apache Spark is an open source cluster computing framework.',\n",
       " '아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " '아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크',\n",
       " \"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'which has maintained it since.',\n",
       " 'Spark provides an interface for programming entire clusters with',\n",
       " 'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c8f1c",
   "metadata": {},
   "source": [
    "# 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13389bf1",
   "metadata": {},
   "source": [
    "myRdd2를 문장으로 나눠봐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fec9cd",
   "metadata": {},
   "source": [
    "split()-> 공백에서 단어를 가른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19037375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Wikipedia'],\n",
       " ['Apache',\n",
       "  'Spark',\n",
       "  'is',\n",
       "  'an',\n",
       "  'open',\n",
       "  'source',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'framework.'],\n",
       " ['아파치', '스파크는', '오픈', '소스', '클러스터', '컴퓨팅', '프레임워크이다.'],\n",
       " ['Apache', 'Spark', 'Apache', 'Spark', 'Apache', 'Spark', 'Apache', 'Spark'],\n",
       " ['아파치', '스파크', '아파치', '스파크', '아파치', '스파크', '아파치', '스파크'],\n",
       " ['Originally',\n",
       "  'developed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'University',\n",
       "  'of',\n",
       "  'California,',\n",
       "  \"Berkeley's\",\n",
       "  'AMPLab,'],\n",
       " ['the',\n",
       "  'Spark',\n",
       "  'codebase',\n",
       "  'was',\n",
       "  'later',\n",
       "  'donated',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Apache',\n",
       "  'Software',\n",
       "  'Foundation,'],\n",
       " ['which', 'has', 'maintained', 'it', 'since.'],\n",
       " ['Spark',\n",
       "  'provides',\n",
       "  'an',\n",
       "  'interface',\n",
       "  'for',\n",
       "  'programming',\n",
       "  'entire',\n",
       "  'clusters',\n",
       "  'with'],\n",
       " ['implicit', 'data', 'parallelism', 'and', 'fault-tolerance.']]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = myRdd2.map(lambda x:x.split())\n",
    "sentence.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e6389",
   "metadata": {},
   "source": [
    "# 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d092c",
   "metadata": {},
   "source": [
    "myRdd2를 문장으로 쪼개고 단어로 쪼갠후 \n",
    "\n",
    "Wikipedia\n",
    "-----\n",
    "Apache Spark is an open source cluster computing framework. \n",
    "-----\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
    "-----\n",
    "Apache Spark A\n",
    "\n",
    "\n",
    "이런 형식으로 만들어라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "499cf4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "===========\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "===========\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "===========\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark \n",
      "===========\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "===========\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "===========\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "===========\n",
      "which has maintained it since. \n",
      "===========\n",
      "Spark provides an interface for programming entire clusters with \n",
      "===========\n",
      "implicit data parallelism and fault-tolerance. \n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "for i in sentence.collect():\n",
    "    for a in i:\n",
    "        print(a, end=' ')\n",
    "       \n",
    "    print('\\n===========')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30856162",
   "metadata": {},
   "source": [
    "아래 문제 다시 풀어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefadeec",
   "metadata": {},
   "source": [
    "# 문제  다시풀어\n",
    "문장별로 문장의 개수를 알아내라 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4c49d",
   "metadata": {},
   "source": [
    "### cf)중요 헷갈리는 부분\n",
    "\n",
    "Rdd에서(파일이 긴 글 일때 ) map을 사용하게 되면 한줄을 읽어서 그 한줄을 하나의 배열에 순서대로 저장한다\n",
    "\n",
    "그후 split()을 해주면 한줄당 각각의 단어로 나뉘어서 추가적인 하나의 리스트로 저장된 2차원 리스트가 만들어진다\n",
    "\n",
    "만일 split(',')을 하면 하나의 문장이 통채로 추가적인 하나의 배열로 2차원 리스트가 만들어진다.\n",
    "\n",
    "기존 rdd에 map을 씌워주면 하나의 문장이 ,이 끝에 오게 나뉜다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ba3b53c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 31, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "myRdd2.map(lambda x : len(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fc70b5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i am'], ['a cow']]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = ['i am', \n",
    "          'a cow']\n",
    "rdd = spark.sparkContext.parallelize(mylist)\n",
    "rdd.map(lambda x : x.split(',')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d06668",
   "metadata": {},
   "source": [
    "## pythonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "df41a2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 2]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=['35','2']\n",
    "[int(i) for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149de9d8",
   "metadata": {},
   "source": [
    "## 대문자, 문자 교체"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bca4a5",
   "metadata": {},
   "source": [
    "# 문제\n",
    "### mylist에서 각 문장에 첫번째 단어를 대문자로 만드시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "964e43d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thLs Ls', 'a lLne']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = ['this is', 'a line']\n",
    "rdd2 = spark.sparkContext.parallelize(mylist)\n",
    "rdd2.map(lambda x : x.replace('i','L')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7be089fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THIS', 'A']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myword = rdd2.map(lambda x: x.split())\n",
    "myword.map(lambda x: x[0].upper()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f322aa",
   "metadata": {},
   "source": [
    "## reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b3495",
   "metadata": {},
   "source": [
    "# 문제 reduce를 이용해 1부터 100까지 숫자의 합을 구하시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6c9920e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(1,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "97d17fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbaa1cc",
   "metadata": {},
   "source": [
    "## filter\n",
    "### 주의: 유니코드 일때는 u를 앞에다가 넣어줘야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "0333f5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9]\n",
    "rdd = spark.sparkContext.parallelize(a)\n",
    "rdd.filter(lambda x : x/1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "399e7809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.filter(lambda x : 'Spark' in x ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "025e5eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " myRdd2.filter(lambda x : u'스파크' in x).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c4f63",
   "metadata": {},
   "source": [
    "### foreach() \n",
    "#### map 과 달리 반환값이 없다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "03cf3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d= spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea96376f",
   "metadata": {},
   "source": [
    "### 파일에 쓰기 rdd파일을 collect()를 써서 리스트로 바꿔줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "49759871",
   "metadata": {},
   "outputs": [],
   "source": [
    "d= spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "d = d.map(lambda x : x).collect()\n",
    "\n",
    "spark.sparkContext.parallelize(d).saveAsTextFile('data/dd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "35d93a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n",
      "5\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/dd/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e459c6",
   "metadata": {},
   "source": [
    "### 로컬에 저장된 텍스트 파일을 읽어서 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "16f1a0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = spark.sparkContext.textFile('data/dd')\n",
    "text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f52718",
   "metadata": {},
   "source": [
    "## 저장된 파일에 새로운 숫자를 쓰려면 coalesce() 함수를 사용해아한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "539e2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.map(lambda x: \"d\".join(x)).coalesce(1).saveAsTextFile('data/dd_n_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "059cd0db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n",
      "5\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/dd_n_txt/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a37581",
   "metadata": {},
   "source": [
    "# Groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefde71a",
   "metadata": {},
   "source": [
    "# 문제\n",
    "### myRdd2에서 앞 두글자로 groupBy 를 시켜라\n",
    "출력 결과\n",
    "\n",
    "Wi: Wikipedia\n",
    "\n",
    "-----\n",
    "\n",
    "Ap: Apache Spark is an open source cluster computing framework.\n",
    "\n",
    "Ap: Apache Spark Apache Spark Apache Spark \n",
    "Apache Spark\n",
    "\n",
    "-----\n",
    "\n",
    "아파: 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "\n",
    "아파: 아파치 스파크 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "73e289cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wi <pyspark.resultiterable.ResultIterable object at 0x7ff8f44e3280>\n",
      "Ap <pyspark.resultiterable.ResultIterable object at 0x7ff8f45cb5e0>\n",
      "아파 <pyspark.resultiterable.ResultIterable object at 0x7ff8f45cbb80>\n",
      "Or <pyspark.resultiterable.ResultIterable object at 0x7ff8f45cbbb0>\n",
      "th <pyspark.resultiterable.ResultIterable object at 0x7ff8f45cb2e0>\n",
      "wh <pyspark.resultiterable.ResultIterable object at 0x7ff8f45cb460>\n",
      "Sp <pyspark.resultiterable.ResultIterable object at 0x7ff8f45e49a0>\n",
      "im <pyspark.resultiterable.ResultIterable object at 0x7ff8f45e4670>\n"
     ]
    }
   ],
   "source": [
    "myRdd2_group = myRdd2.groupBy(lambda x: x[0:2])\n",
    "type(myRdd2_group)\n",
    "for k,y in myRdd2_group.collect():\n",
    "    print(k,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ad192",
   "metadata": {},
   "source": [
    "## ResultIterable은 반복문으로 해체시켜야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c7756610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wi: Wikipedia\n",
      "Ap: Apache Spark is an open source cluster computing framework.\n",
      "Ap: Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "아파: 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아파: 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "Or: Originally developed at the University of California, Berkeley's AMPLab,\n",
      "th: the Spark codebase was later donated to the Apache Software Foundation,\n",
      "wh: which has maintained it since.\n",
      "Sp: Spark provides an interface for programming entire clusters with\n",
      "im: implicit data parallelism and fault-tolerance.\n"
     ]
    }
   ],
   "source": [
    "for k,v in myRdd2_group.collect():\n",
    "    for each in v:\n",
    "        print(\"{}: {}\".format(k,each))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "9631438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"Seoul\",1),(\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9570a825",
   "metadata": {},
   "source": [
    "## ('Seoul',1)이건 paired 데이터임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c90c9287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seoul: ('Seoul', 1)\n",
      "Seoul: ('Seoul', 1)\n",
      "Seoul: ('Seoul', 1)\n",
      "Seoul: ('Seoul', 1)\n",
      "Seoul: ('Seoul', 1)\n",
      "Seoul: ('Seoul', 1)\n",
      "Busan: ('Busan', 1)\n",
      "Busan: ('Busan', 1)\n",
      "Busan: ('Busan', 1)\n",
      "Busan: ('Busan', 1)\n",
      "Busan: ('Busan', 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Seoul',\n",
       "  [('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1),\n",
       "   ('Seoul', 1)]),\n",
       " ('Busan',\n",
       "  [('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1), ('Busan', 1)])]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd3 = spark.sparkContext.parallelize(_testList)\n",
    "myRdd3_group = myRdd3.groupBy(lambda x : x[0])\n",
    "\n",
    "for k,v in myRdd3_group.collect():\n",
    "    for each in v:\n",
    "        print(\"{}: {}\".format(k,each))\n",
    "        \n",
    "myRdd3_group.mapValues(lambda x: list(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a223d",
   "metadata": {},
   "source": [
    "ResultIterable 변환 하기 \n",
    "1. 반복문사용\n",
    "2. mapValues 사용\n",
    "위 코드를 해석하면 mapValues쓸때 key에 대한 값이 들어갔다. \n",
    "\n",
    "따라서 'seoul'과 seoul이 들어간 paired 데이터들을 리스트로 저장헀다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70729703",
   "metadata": {},
   "source": [
    "# pairedRdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c2270",
   "metadata": {},
   "source": [
    "### group by key보다는 reducebykey를써라 \n",
    "### glom 은 파티션의 개수 알수 있음\n",
    "### mapValue는 pairedRdd의 key value값에서 value에 적용하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c179c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "64e48bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "0b4d19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_testRdd=spark.sparkContext.parallelize(_testList, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44adbe0",
   "metadata": {},
   "source": [
    "##### (_testList, 2) 이거는 _testList라는 RDD 에 파티션을 2로 늘리자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6ae97062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85630ef5",
   "metadata": {},
   "source": [
    "#### glom() 함수는 각 partition에 있는 요소를 묶어서 RDD 만들어 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542d0ed",
   "metadata": {},
   "source": [
    "enumerate -> 인덱스가 몇번째로 되어있는지 알아본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "80ed00cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions 0 -> [('key1', 1), ('key1', 1), ('key1', 1), ('key2', 1), ('key2', 1)]\n",
      "Partitions 1 -> [('key1', 1), ('key2', 1), ('key1', 1), ('key1', 1), ('key2', 1), ('key2', 1)]\n"
     ]
    }
   ],
   "source": [
    "partitions = _testRdd.glom().collect()\n",
    "for num, partition in enumerate(partitions):\n",
    "     print(f'Partitions {num} -> {partition}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc229f00",
   "metadata": {},
   "source": [
    "## groupByKey, reduceByKey, mapValues\n",
    "\n",
    "#### reduceByKey 써라\n",
    "동일한 key에 대해 value를 합계한다. \n",
    "\n",
    "#### groupByKey \n",
    "partition별로 수행되지 않고 메모리에 값을 모두 저장해 놓는다.\n",
    "\n",
    "결과는 ResultIterable로 바로 결과를 볼 수 없다.\n",
    "\n",
    "**```mapValues()```**함수에 **list() 함수**를 적어주고 collect()하면 그 결과를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "2998ed50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 6), ('key2', 5)]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "3c14871a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', <pyspark.resultiterable.ResultIterable at 0x7ff8f45d42e0>),\n",
       " ('key2', <pyspark.resultiterable.ResultIterable at 0x7ff8f45dbd00>)]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "4f714067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', [1, 1, 1, 1, 1, 1]), ('key2', [1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9323972d",
   "metadata": {},
   "source": [
    "### mapValues 는 값(value)만 변동을 준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "b71813c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2)]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.mapValues(lambda x:x*2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1eeae",
   "metadata": {},
   "source": [
    "### flatMap()은 리스트안에 또 리스트가 있는 경우 이를 하나의 리스트로 만든다\n",
    "\n",
    "map()은 요소별로 적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "ddb49a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', <pyspark.resultiterable.ResultIterable at 0x7ff8f45fef10>),\n",
       " ('Apache', <pyspark.resultiterable.ResultIterable at 0x7ff8f45fedc0>)]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.flatMap(lambda x: x.split()).map(lambda x : (x,1))\\\n",
    "        .groupByKey()\\\n",
    "        .take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "77089c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', 1),\n",
       " ('Apache', 6),\n",
       " ('Spark', 7),\n",
       " ('is', 1),\n",
       " ('an', 2),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('framework.', 1),\n",
       " ('아파치', 5),\n",
       " ('스파크는', 1),\n",
       " ('오픈', 1),\n",
       " ('소스', 1),\n",
       " ('클러스터', 1),\n",
       " ('컴퓨팅', 1),\n",
       " ('프레임워크이다.', 1),\n",
       " ('스파크', 4),\n",
       " ('Originally', 1),\n",
       " ('developed', 1)]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "1bbabace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AMPLab,', 1),\n",
       " ('Apache', 6),\n",
       " (\"Berkeley's\", 1),\n",
       " ('California,', 1),\n",
       " ('Foundation,', 1),\n",
       " ('Originally', 1),\n",
       " ('Software', 1),\n",
       " ('Spark', 7),\n",
       " ('University', 1),\n",
       " ('Wikipedia', 1)]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x): return len(x)\n",
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(f)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a50621a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc=myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "c50ea153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어:AMPLab,\t\t빈도:1\n",
      "단어:Apache\t\t빈도:6\n",
      "단어:Berkeley's\t\t빈도:1\n",
      "단어:California,\t\t빈도:1\n",
      "단어:Foundation,\t\t빈도:1\n",
      "단어:Originally\t\t빈도:1\n",
      "단어:Software\t\t빈도:1\n",
      "단어:Spark\t\t빈도:7\n",
      "단어:University\t\t빈도:1\n",
      "단어:Wikipedia\t\t빈도:1\n"
     ]
    }
   ],
   "source": [
    "for e in wc:\n",
    "    k = e[0]\n",
    "    v = e[1]\n",
    "    print (f\"단어:{k}\\t\\t빈도:{v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703e981",
   "metadata": {},
   "source": [
    "#### countByKey\n",
    "\n",
    "**```countByKey()```**는 Key별로 계산을 하는 action 함수이고, 그 결과는 **dictionary**로 출력된다.\n",
    "**```coutByKey().items()```**하면 리스트로 변환될 수 있다.\n",
    "\n",
    "```countByKey()```는 dictionary로 병합하는 반면, reduceByKey()는 (K,V)로 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "56909a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Wikipedia': 1,\n",
       "             'Apache': 6,\n",
       "             'Spark': 7,\n",
       "             'is': 1,\n",
       "             'an': 2,\n",
       "             'open': 1,\n",
       "             'source': 1,\n",
       "             'cluster': 1,\n",
       "             'computing': 1,\n",
       "             'framework.': 1,\n",
       "             '아파치': 5,\n",
       "             '스파크는': 1,\n",
       "             '오픈': 1,\n",
       "             '소스': 1,\n",
       "             '클러스터': 1,\n",
       "             '컴퓨팅': 1,\n",
       "             '프레임워크이다.': 1,\n",
       "             '스파크': 4,\n",
       "             'Originally': 1,\n",
       "             'developed': 1,\n",
       "             'at': 1,\n",
       "             'the': 3,\n",
       "             'University': 1,\n",
       "             'of': 1,\n",
       "             'California,': 1,\n",
       "             \"Berkeley's\": 1,\n",
       "             'AMPLab,': 1,\n",
       "             'codebase': 1,\n",
       "             'was': 1,\n",
       "             'later': 1,\n",
       "             'donated': 1,\n",
       "             'to': 1,\n",
       "             'Software': 1,\n",
       "             'Foundation,': 1,\n",
       "             'which': 1,\n",
       "             'has': 1,\n",
       "             'maintained': 1,\n",
       "             'it': 1,\n",
       "             'since.': 1,\n",
       "             'provides': 1,\n",
       "             'interface': 1,\n",
       "             'for': 1,\n",
       "             'programming': 1,\n",
       "             'entire': 1,\n",
       "             'clusters': 1,\n",
       "             'with': 1,\n",
       "             'implicit': 1,\n",
       "             'data': 1,\n",
       "             'parallelism': 1,\n",
       "             'and': 1,\n",
       "             'fault-tolerance.': 1})"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .countByKey() # .items() to be added to get a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432cd2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
