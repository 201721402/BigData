{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "dc628f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a108d7b",
   "metadata": {},
   "source": [
    "### S.4.1 schema 생성하기\n",
    "\n",
    "DataFrame은 데이터 모델 schema를 정의하고, 각 컬럼의 명칭과 데이터타입을 정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3e76513c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2', '_3']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList=[('1','kim, js', 170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee', 170)]\n",
    "myDf=spark.cqreateDataFrame(myList)\n",
    "myDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e806b551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1='1', _2='kim, js', _3=170), Row(_1='1', _2='lee, sm', _3=175), Row(_1='2', _2='lim, yg', _3=180)]\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "print (myDf.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73dbf3e",
   "metadata": {},
   "source": [
    "#### 컬럼명 설정\n",
    "\n",
    "앞서 컬럼 Column을 정의하지 않고 DataFrame을 생성하였는데, 이번에는 **컬럼을 정해서** 생성하자.\n",
    "**```createDataFrame()```** 함수에 **인자로 컬럼명을 리스트 ```['year','name','height']```로** 정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7d15a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n",
      "[Row(year='1', name='kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "cols = ['year','name','height']\n",
    "_myDf = spark.createDataFrame(myList, cols)\n",
    "_myDf.show()\n",
    "print (_myDf.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e767d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "40b18cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='kim', coffee='espresso')]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#아래 해석 총 100개를 뽑는데 위에 스키마 크기만큼 계속 할당시킴 \n",
    "\n",
    "coffeeDf = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"coffee\"])\n",
    "coffeeDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "56e1b0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- coffee: string (nullable = true)\n",
      "\n",
      "+----+----------+\n",
      "|name|    coffee|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "| lee|long black|\n",
      "| lim|  macciato|\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "| lee|long black|\n",
      "| lim|  macciato|\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "| lee|long black|\n",
      "| lim|  macciato|\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "| lee|long black|\n",
      "| lim|  macciato|\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "| lee|long black|\n",
      "| lim|  macciato|\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "| lee|long black|\n",
      "| lim|  macciato|\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "| lee|long black|\n",
      "| lim|  macciato|\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "| lee|long black|\n",
      "| lim|  macciato|\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.printSchema()\n",
    "coffeeDf.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbccd804",
   "metadata": {},
   "source": [
    " # Row 객체를 사용해서 생성 후 dataframe 생성\n",
    "\n",
    "* Row 생성\n",
    "**Row**를 사용해 보자.\n",
    "Row는 **이름(Column)이 붙여진 행**으로 **관계형데이터베이스 레코드 Record**와 같다. \n",
    "속성 명은 'year', 'name', 'height'로 명명한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1814a41",
   "metadata": {},
   "source": [
    "row는 어떠한 카테고리로 나눌지 결정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b83289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "#row 생성\n",
    "Person = Row('year','name','height')\n",
    "\n",
    "#해당 row 형식에 맞게 데이터를 젛어줘서 schema 를 생성해라 \n",
    "myRows = [Person('1','lee, sm', 175),\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b3362ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf = spark.createDataFrame(myRows)\n",
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7948453",
   "metadata": {},
   "source": [
    "# schema를 정의하고 생성 후 dataFrame 에 넣기\n",
    "\n",
    "모델 schema를 정하고, **데이터 타잎**을 정의해 DataFrame을 생성해 본다.\n",
    "**```StructType```**으로 구조체를 선언하고, 컬럼에 대해 **```StructField```**를 설정한다.\n",
    "* **컬럼**의 명칭\n",
    "* 앞서 소개했던 **데이터 타잎**\n",
    "* 마지막은 **NULL**이 허용되는지 여부\n",
    "\n",
    "```python\n",
    "StructType([\n",
    "    StructField(컬럼명, StringType(), True),\n",
    "    ...\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "945a687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b74bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(year='1', name='kim, js', height=170)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema)\n",
    "myDf.printSchema()\n",
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d23f4",
   "metadata": {},
   "source": [
    "# S.4.2 RDD에서 생성하기\n",
    "\n",
    "RDD는 schema가 정해지지 않은 비구조적 데이터이다.\n",
    "이와 같이 **schema를 정의하지 않으면, Spark는 schema를 유추**하게 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e4c3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "677abb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| _1|     _2| _3|\n",
      "+---+-------+---+\n",
      "|  1|kim, js|170|\n",
      "|  1|lee, sm|175|\n",
      "|  2|lim, yg|180|\n",
      "|  2|    lee|170|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myList=[('1','kim, js',170), \n",
    "        ('1','lee, sm', 175), \n",
    "        ('2','lim, yg',180), \n",
    "        ('2','lee',170)]\n",
    "myrdd = spark.sparkContext.parallelize(myList)\n",
    "mydf = myrdd.toDF()\n",
    "mydf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486ab64",
   "metadata": {},
   "source": [
    "### row 사용하여 행생성\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6efcf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#예제 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "37c8532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "_myrdd = myrdd.map(lambda x : Row(year = int(x[0]), name = x[1], height = x[2]))\n",
    "real = spark.createDataFrame(_myrdd)\n",
    "real.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a043bb",
   "metadata": {},
   "source": [
    "## S.4.3 Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637d666",
   "metadata": {},
   "source": [
    "### Dataframe을 Pandas로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4d3f2f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_1</th>\n",
       "      <th>_2</th>\n",
       "      <th>_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  _1       _2   _3\n",
       "0  1  kim, js  170\n",
       "1  1  lee, sm  175\n",
       "2  2  lim, yg  180\n",
       "3  2      lee  170"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a922c",
   "metadata": {},
   "source": [
    "### Pandas에서 csv 파일로 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0a85fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mydf.write.format('com.databricks.spark.csv').save(os.path.join('data','dd.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "17a166cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "-rw-r--r--  1 nadonghyeon  staff   0 Oct 22 18:26 _SUCCESS\r\n",
      "-rw-r--r--  1 nadonghyeon  staff  58 Oct 22 18:26 part-00000-d60b15c3-0ca3-4ddb-8494-01357c16d56b-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/dd.csv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f886a56",
   "metadata": {},
   "source": [
    "# Pandas에서 컬럼을 생성,삭제 해보자.\n",
    "recode - 현재 변수 값을 다시 줄 경우\n",
    "\n",
    "나라변 국제전화코드는 Japan: 81, South Korea: 82, Hong Kong: 852, Australia: 61을 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce7c2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json 형식으로 만듬\n",
    "import pandas as pd\n",
    "icc = pd.DataFrame( { 'country': ['South Korea','Japan','Hong Kong'],'codes': [81, 82, 852] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cab5efbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  codes\n",
       "0  South Korea     81\n",
       "1        Japan     82\n",
       "2    Hong Kong    852"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5fc328",
   "metadata": {},
   "source": [
    "### S.4.4 csv 파일에서 생성\n",
    "\n",
    "#### RDD에서 DataFrame\n",
    "\n",
    "앞서 RDD에서 읽었던 csv파일을 다시 읽어보자.\n",
    "\n",
    "```sparkContext.textFile()``` 함수로 읽은 파일은 RDD이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dfaa1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip => 앞 뒤에 관련 없는 단어가 들어오는거 방지하기 위해\n",
    "# ex ) ad. 와 ad를 같은 단어로 인식한다. 따라서 strip()을 하면서 단어 제외시켜라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b74d5680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|  35|   2|\n",
      "|  40|  27|\n",
      "|  12|  38|\n",
      "|  15|  31|\n",
      "|  21|   1|\n",
      "|  14|  19|\n",
      "|  46|   1|\n",
      "|  10|  34|\n",
      "|  28|   3|\n",
      "|  48|   1|\n",
      "|  16|   2|\n",
      "|  30|   3|\n",
      "|  32|   2|\n",
      "|  48|   1|\n",
      "|  31|   2|\n",
      "|  22|   1|\n",
      "|  12|   3|\n",
      "|  39|  29|\n",
      "|  19|  37|\n",
      "|  25|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(\"data\", \"ds_spark_2cols.csv\")\n",
    "lines = spark.sparkContext.textFile(cfile)\n",
    "_col12 = line.map(lambda x: x.split(','))\n",
    "col12 = new.map(lambda x: Row(col1 = int(x[0]), col2 = int(x[1])))\n",
    "_myDf = spark.createDataFrame(col12)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050323a",
   "metadata": {},
   "source": [
    "#### DataFrame으로 직접 읽기\n",
    "\n",
    "format().load() 또는 csv() 함수로 csv 파일을 읽어서 DataFrame을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78e66088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0a522c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferschema='true', delimiter=',')\\\n",
    "        .load(os.path.join('data','ds_spark.csv'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444449a9",
   "metadata": {},
   "source": [
    "#### csv\n",
    "\n",
    "또는 \n",
    "```csv(\"path\")```로 직접 DataFrame으로 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b91cbb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .options(header='true', inferschema='true', delimiter=',')\\\n",
    "        .csv(os.path.join('data','ds_spark.csv'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d8606",
   "metadata": {},
   "source": [
    "### S.4.5 tsv 파일 읽기 (제대로 된 예제)\n",
    "\n",
    "tsv (tab-separated values)는 **Tab으로 분리된 파일**을 말한다.\n",
    "'\\t'이 포함되어 있는 경우, 혹시 string으로 데이터타잎을 설정하기도 한다 (과거 Spark 버전에서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f3931bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#예제1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3edc0ac",
   "metadata": {},
   "source": [
    "# 이거 다시 푸어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc06ad5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-87-6ffbf930edea>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-87-6ffbf930edea>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    1\t65.78\t112.99\u001b[0m\n\u001b[0m     \t^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# %load data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f081a",
   "metadata": {},
   "source": [
    "경로 'data','ds_spark_heightweight.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081e18f2",
   "metadata": {},
   "source": [
    "_rdd = rdd.map(lambda x : [float(x) for x in x.split('\\t')]) 이거 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fd7f41ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|value|weight|height|\n",
      "+-----+------+------+\n",
      "|  1.0| 65.78|112.99|\n",
      "|  2.0| 71.52|136.49|\n",
      "|  3.0|  69.4|153.03|\n",
      "|  4.0| 68.22|142.34|\n",
      "|  5.0| 67.79| 144.3|\n",
      "|  6.0|  68.7| 123.3|\n",
      "|  7.0|  69.8|141.49|\n",
      "|  8.0| 70.01|136.46|\n",
      "|  9.0|  67.9|112.37|\n",
      "| 10.0| 66.78|120.67|\n",
      "| 11.0| 66.49|127.45|\n",
      "| 12.0| 67.62|114.14|\n",
      "| 13.0|  68.3|125.61|\n",
      "| 14.0| 67.12|122.46|\n",
      "| 15.0| 68.28|116.09|\n",
      "| 16.0| 71.09| 140.0|\n",
      "| 17.0| 66.46| 129.5|\n",
      "| 18.0| 68.65|142.97|\n",
      "| 19.0| 71.23| 137.9|\n",
      "| 20.0| 67.13|124.04|\n",
      "+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "file = os.path.join('data','ds_spark_heightweight.txt')\n",
    "_tRdd= spark.sparkContext.textFile(file)\n",
    "tRdd = _tRdd.map(lambda x : [float(x) for x in x.split('\\t')])\n",
    "tDfNamed = spark.createDataFrame(tRdd, ['value','weight','height'])\n",
    "tDfNamed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc21d93",
   "metadata": {},
   "source": [
    "## text 함수를 이용해 파일 읽음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d5d66216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+------+\n",
      "|          value|weight|height|\n",
      "+---------------+------+------+\n",
      "| 1\t65.78\t112.99| 65.78|112.99|\n",
      "| 2\t71.52\t136.49| 71.52|136.49|\n",
      "| 3\t69.40\t153.03| 69.40|153.03|\n",
      "| 4\t68.22\t142.34| 68.22|142.34|\n",
      "| 5\t67.79\t144.30| 67.79|144.30|\n",
      "| 6\t68.70\t123.30| 68.70|123.30|\n",
      "| 7\t69.80\t141.49| 69.80|141.49|\n",
      "| 8\t70.01\t136.46| 70.01|136.46|\n",
      "| 9\t67.90\t112.37| 67.90|112.37|\n",
      "|10\t66.78\t120.67| 66.78|120.67|\n",
      "|11\t66.49\t127.45| 66.49|127.45|\n",
      "|12\t67.62\t114.14| 67.62|114.14|\n",
      "|13\t68.30\t125.61| 68.30|125.61|\n",
      "|14\t67.12\t122.46| 67.12|122.46|\n",
      "|15\t68.28\t116.09| 68.28|116.09|\n",
      "|16\t71.09\t140.00| 71.09|140.00|\n",
      "|17\t66.46\t129.50| 66.46|129.50|\n",
      "|18\t68.65\t142.97| 68.65|142.97|\n",
      "|19\t71.23\t137.90| 71.23|137.90|\n",
      "|20\t67.13\t124.04| 67.13|124.04|\n",
      "+---------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "tDftxt = spark.read.text(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "split = split(tDftxt['value'],'\\t')\n",
    "tDftxt = tDftxt.withColumn('weight',split.getItem(1))\n",
    "tDftxt = tDftxt.withColumn('height',split.getItem(2))\n",
    "tDftxt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "4be27315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'split(value, \t, -1)[1]'>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split.getItem(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147f172",
   "metadata": {},
   "source": [
    "## csv 함수로 tsv 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "48caedf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|_c0|  _c1|   _c2|\n",
      "+---+-----+------+\n",
      "|  1|65.78|112.99|\n",
      "|  2|71.52|136.49|\n",
      "|  3|69.40|153.03|\n",
      "|  4|68.22|142.34|\n",
      "|  5|67.79|144.30|\n",
      "|  6|68.70|123.30|\n",
      "|  7|69.80|141.49|\n",
      "|  8|70.01|136.46|\n",
      "|  9|67.90|112.37|\n",
      "| 10|66.78|120.67|\n",
      "| 11|66.49|127.45|\n",
      "| 12|67.62|114.14|\n",
      "| 13|68.30|125.61|\n",
      "| 14|67.12|122.46|\n",
      "| 15|68.28|116.09|\n",
      "| 16|71.09|140.00|\n",
      "| 17|66.46|129.50|\n",
      "| 18|68.65|142.97|\n",
      "| 19|71.23|137.90|\n",
      "| 20|67.13|124.04|\n",
      "+---+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='false', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'ds_spark_heightweight.txt'))\n",
    "tDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ddbfc",
   "metadata": {},
   "source": [
    "### S.4.6 JSON 파일에서 생성\n",
    "\n",
    "#### JSON\n",
    "JSON은 JavaScript Object Notation, 즉 자바스크립트에서 사용되는 표기법. 사람이 읽을 수 있는 텍스트로 표기하며, key-value 쌍으로 되어 있다.\n",
    "현재 널리 쓰이고 있어 XML 대용으로 널리 쓰이고 있다.\n",
    "Spark example 폴더에 있는 ```people.json``` JSON 파일이다.\n",
    "\n",
    "```python\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6de523",
   "metadata": {},
   "source": [
    "아래 인스타그램 데이터는 되지를 않음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "648463e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_twitter_seoul_3.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_twitter_seoul_3.json\n",
    "\n",
    "\n",
    "\n",
    "{\"contributors\": null, \n",
    " \"truncated\": false, \n",
    " \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\", \n",
    " \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \n",
    " \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \n",
    "              \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \n",
    "              \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}]}, \n",
    " \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \n",
    " \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \n",
    "                      \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \n",
    "                                                                                                                    \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}], \n",
    "                                                                                                                    \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \n",
    "                                                                                                                               \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \n",
    "                                                                                                                               \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \n",
    "                                                                                                                               \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \n",
    "                                                                                                                                \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \n",
    "                                                                                                                               \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \n",
    "                      \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \n",
    "                      \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \n",
    "                                                   \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias' fashion choices seriously. But not rumors. Ain't nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "62966117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json파일이라도 모두가 문자열 \n",
    "# 여기에 의미를 부여한다 -> 파싱한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "61cfcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_jfname=os.path.join('src','ds_twitter_seoul_3.json')\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "643733e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 2 column 1 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-dc8c4caf8f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_json_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 1)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data_json_str = json.loads(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b655e",
   "metadata": {},
   "source": [
    "# pandas 에서 트윗 읽기는 보지마 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd26d9c",
   "metadata": {},
   "source": [
    "# dataFrame 에서 json 데이터 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "55a870a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jfile= os.path.join('src','ds_twitter_seoul_3.json')\n",
    "\n",
    "tweetDf= spark.read.json(jfile)\n",
    "tweetDf.printSchema()\n",
    "tweetDf.count()\n",
    "tweetDf.select('id', 'text').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439eebee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
